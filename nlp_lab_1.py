# -*- coding: utf-8 -*-
"""nlp lab 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14L-dle2giHWOaHxOhrMoTHRyIlP16rjQ
"""

import pandas as pd
import numpy as np
import nltk



from google.colab import files
uploaded = files.upload()

data = pd.read_csv('spam.csv', encoding='latin1')

data.head(10)

data.columns = ['label', 'message', 'Unnamed1', 'Unnamed2', 'Unnamed3']
df = data[['label', 'message']]

df.head(10)

df.isnull().sum()

num_duplicates = df.duplicated().sum()
num_duplicates, df.shape

df = df.drop_duplicates(keep='first')
df.shape

import matplotlib.pyplot as plt


value_counts = df['label'].value_counts()
value_counts


plt.figure(figsize=(8, 6))
ax = value_counts.plot(kind='bar', color=['blue', 'red'])
plt.title('Distribution of Target col')
plt.xlabel('Labels')
plt.ylabel('Count')

for container in ax.containers:
    ax.bar_label(container, label_type='edge')

plt.show()

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re
from wordcloud import WordCloud
nltk.download('punkt')

def preprocess(text):
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    return tokens

df['tokens'] = df['message'].apply(preprocess)

import nltk
nltk.download('stopwords')







ham_words = Counter([word for tokens in df[df['label'] == 'ham']['tokens'] for word in tokens])
spam_words = Counter([word for tokens in df[df['label'] == 'spam']['tokens'] for word in tokens])

print("10 Most Frequent Words in Ham Messages:")
print(ham_words.most_common(10))

print("\n10 Most Frequent Words in Spam Messages:")
print(spam_words.most_common(10))

from nltk.util import ngrams

def get_ngrams(tokens_list, n):
    ngrams_list = [ngrams(tokens, n) for tokens in tokens_list]
    ngrams_flat = [item for sublist in ngrams_list for item in sublist]
    return Counter(ngrams_flat)

# Find bigrams
ham_bigrams = get_ngrams(df[df['label'] == 'ham']['tokens'], 2)
spam_bigrams = get_ngrams(df[df['label'] == 'spam']['tokens'], 2)

print("10 Most Frequent Bigrams in Ham Messages:")
print(ham_bigrams.most_common(10))

print("\n10 Most Frequent Bigrams in Spam Messages:")
print(spam_bigrams.most_common(10))

# Find trigrams
ham_trigrams = get_ngrams(df[df['label'] == 'ham']['tokens'], 3)
spam_trigrams = get_ngrams(df[df['label'] == 'spam']['tokens'], 3)

print("10 Most Frequent Trigrams in Ham Messages:")
print(ham_trigrams.most_common(10))

print("\n10 Most Frequent Trigrams in Spam Messages:")
print(spam_trigrams.most_common(10))

def generate_wordcloud(text_series, title):
    text = ' '.join(text_series)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

generate_wordcloud(df[df['label'] == 'spam']['clean_message'], 'Spam Messages Word Cloud')
generate_wordcloud(df[df['label'] == 'ham']['clean_message'], 'Ham Messages Word Cloud')



vectorizer = CountVectorizer()
X_bow = vectorizer.fit_transform(df['message'])
y = df['label']



from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score


X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.3, random_state=42)


model = MultinomialNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


f1_bow = f1_score(y_test, y_pred, average='weighted')
print("F1 Score (BoW):", f1_bow)

from sklearn.feature_extraction.text import TfidfVectorizer


tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(df['message'])


X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

f1_tfidf = f1_score(y_test, y_pred, average='weighted')
print("F1 Score (Tf-IDF):", f1_tfidf)



from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

def advanced_preprocess(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

df['clean_message'] = df['message'].apply(advanced_preprocess)



X_tfidf_clean = tfidf_vectorizer.fit_transform(df['clean_message'])


X_train, X_test, y_train, y_test = train_test_split(X_tfidf_clean, y, test_size=0.3, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


f1_tfidf_clean = f1_score(y_test, y_pred, average='weighted')
print("F1 Score (Tf-IDF with Preprocessing):", f1_tfidf_clean)

print("F1 Score (BoW):", f1_bow)
print("F1 Score (Tf-IDF):", f1_tfidf)
print("F1 Score (Tf-IDF with Preprocessing):", f1_tfidf_clean)

